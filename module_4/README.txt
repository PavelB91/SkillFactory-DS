1. Описание проекта:
    Проект выполнен на Kaggle:
    Автор: Pavel Begunov (https://www.kaggle.com/pavelbegunov)
    Ссылка на ноутбук на Kaggle: https://www.kaggle.com/pavelbegunov/pavel-begunov-sf-credit-scoring
    Достигнутое в соревновании значении метрики ROC_AUC: 
        1) Максимальное значение: 0.73650
        2) На котором решил остановиться: 0.73088
        
2. Цель: 
    Построить скоринговую модель предсказания дефолта клиентов банка.
Задачи:
    1) Провести предварительную очистку данных.
    2) Провести оценку и преобразование данных.
    3) Сформулировать выводы относительно качества данных и тех признаков, которые будут использоваться в дальнейшем построении модели.
    4) Построить и обучить модель. Найти наилучшие параметры модели.
    5) Использовать различные метрики для оценки качества модели.
    
3. Краткая информация о данных:
    Датасет содержит информацию из анкетных данных о заемщиках, которые уже брали кредиты (повторных клиентов) и факт наличия дефолта.
    Первоначальная версия датасета состоит из девятнадцати столбцов, содержащих следующую информацию:
    * client_id: идентификатор клиента;
    * education: уровень образования;
    * sex: пол заёмщика;
    * age: возраст заёмщика;
    * car: флаг наличия автомобиля;
    * car_type: флаг автомобиля-иномарки;
    * decline_app_cnt: количесвто отказанных прошлых заявок;
    * good_work: флаг наличия "хорошей" работы;
    * bki_request_cnt: количество запросов в БКИ(Бюро кредитных историй);
    * home_adress: категоризатор домашнего адреса;
    * work_adress: категоризатор рабочего адреса;
    * income: доход заёмщика;
    * foreign_passport: наличие загранпаспорта;
    * sna: связь заемщика с клиентами банка;
    * first_time: давность наличия информации о заемщике;
    * score_bki: скоринговый балл по данным из БКИ;
    * region_rating: рейтинг региона;
    * app_date: дата подачи заявки;
    * default: наличие дефолта;
    
4. Этапы работы над проектом.
    1) Был проведен первичный анализ данных. В котором были определены типы данных, количество строк и столбцов, наличие пропусков и дубликатов.
    2) Проведен анализ данных по группам признаков.
    3) Заполнены пропуски и минимизированы выбросы.
    4) Созданы новые признаки на основе имеющихся данных.
    5) Отобраны признаки, которые влияют на конечный результат.
    6) Данные преобразованы в вид, пригодный для модели.
    7) Построена и обчена модель, подобраны гиперпараметры.
    8) Использованы различные метрики для оценки качества модели.
    
5. Особенности работы:
    1) Работу выполнял один.
    2) Во время работы прибегнул к использованию:
        * Различных методов кодирования данных из библиотеки sklearn.preprocessing: LabelEncoder, OneHotEncoder.
        * Методу PCA для декомпозиции сильноскоррелированных признаков.
        * Различным видам стандартизации данных: StandardScaler*, MinMaxScaler, RobustScaler.
        * Различным методам Undersampling и Oversampling* для балансировки данных.
        * Различным метрикам, которые применимы к задачам классификации.
        * Подбору гиперпараметров для модели с помощью GridSearchCV.
    3) Получил максимальное значение ROC_AUC в четвертой версии ноутбука, однако все остальные метрики имели очень малые значения, что указало на слабость модели. Поэтому решил сделать модель более рабочей (шестая версия), хоть это и снизило значение метрики ROC_AUC. Но несмотря на это значения всех остальных метрик много увеличились, и значимо снизилась ошибка 2 рода.
    * - методы, которые в итоге использовались. Для Oversamling лучше всего показал себя метод **ADASYN**.